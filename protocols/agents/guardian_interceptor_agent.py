# protocols/guardian_interceptor_agent.py

"""GuardianInterceptorAgent â€“ CI/CD defense against hallucinated patches.

This module defines :class:`GuardianInterceptorAgent`, an internal agent that
listens for LLM-suggested code edits, evaluates them for risk, and proposes safe
alternatives.  The agent retains the original logic but is organized for easier
extension.  Hooks are provided for integration with other validators like the
``MetaValidatorAgent`` or ``PatchReviewer``.

The agent can optionally leverage an ``llm_backend`` callable for deeper text
analysis when inspecting or proposing fixes.
"""

from typing import Callable, Dict, List
import uuid
import time  # retained for potential future hooks

from protocols.core.internal_protocol import InternalAgentProtocol


# ---------------------------------------------------------------------------
# Risk pattern definitions
# ---------------------------------------------------------------------------

RISK_PATTERNS = {
    # Each entry maps a human-readable flag to a boolean predicate executed
    # against ``suggestion.lower()``.
    "Destructive command without context": lambda s: "delete" in s and "import" not in s,
    "Security bypass term detected": lambda s: "bypass" in s,
    "Force-fail logic included": lambda s: "assert false" in s,
    "No justification for fix": lambda s: "fix" in s and "reason" not in s,
}


class GuardianInterceptorAgent(InternalAgentProtocol):
    """Intercepts LLM suggestions and guards CI/CD integrity.

    Parameters
    ----------
    llm_backend : callable, optional
        Optional function used to perform advanced analysis of suggestions. If
        omitted, built-in heuristics operate without LLM assistance.
    """

    def __init__(self, llm_backend: Callable[[str], str] | None = None) -> None:
        super().__init__()
        self.name = "GuardianInterceptor"
        self.llm_backend = llm_backend
        self.receive("LLM_INCOMING", self.inspect_suggestion)
        self.receive("REQUEST_PATCH_PROPOSAL", self.propose_fix)
        # TODO: integrate with MetaValidatorAgent for second-opinion audits

    # ------------------------------------------------------------------
    # Event Handlers
    # ------------------------------------------------------------------
    def inspect_suggestion(self, payload: Dict[str, str]) -> Dict[str, any]:
        """Analyze an incoming LLM suggestion and emit a risk judgment."""

        suggestion = payload.get("content", "")
        llm_id = payload.get("llm_id", str(uuid.uuid4()))

        if self.llm_backend:
            suggestion = self.llm_backend(suggestion)

        red_flags = self._detect_risks(suggestion)
        judgment = self._build_judgment(llm_id, suggestion, red_flags)

        self.send("LLM_EVALUATION_RESULT", judgment)
        return judgment

    def propose_fix(self, payload: Dict[str, str]) -> Dict[str, any]:
        """Return a basic safe patch for a reported issue."""

        issue = payload.get("issue", "Unclear bug")
        context = payload.get("context", "")

        if self.llm_backend:
            prompt = (
                "Provide a safe patch for the following issue:" f" {issue}\n{context}"
            )
            fix_code = self.llm_backend(prompt)
        else:
            fix_code = (
                f"# Auto-generated patch to address: {issue}\n"
                'print("[Fix applied]")\n'
                f"# Context: {context[:60]}"
            )

        # TODO: Send to PatchReviewer agent before applying automatically
        return {
            "patch": fix_code,
            "comment": "Proposed safe fix generated by GuardianInterceptor.",
            "confidence": 0.85,
            "next_step": "Await confirmation or peer review.",
        }

    # ------------------------------------------------------------------
    # Helper utilities
    # ------------------------------------------------------------------
    def _detect_risks(self, suggestion: str) -> List[str]:
        """Return a list of descriptive flags for risky content."""

        text = suggestion.lower()
        return [label for label, rule in RISK_PATTERNS.items() if rule(text)]

    @staticmethod
    def _build_judgment(llm_id: str, suggestion: str, flags: List[str]) -> Dict[str, any]:
        """Create the judgment structure returned to the message bus."""

        return {
            "llm_id": llm_id,
            "content_preview": suggestion[:60],
            "risk_level": "HIGH" if flags else "LOW",
            "flags": flags,
            "approved": not flags,
        }


# Example usage:
# guardian = GuardianInterceptorAgent()
# guardian.send("LLM_INCOMING", {"content": "delete everything in /tmp"})
